{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import threading\n",
    "import time\n",
    "# from playsound import playsound \n",
    "import winsound\n",
    "# from pydub import AudioSegment\n",
    "# from pydub.playback import play  \n",
    "#import keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_sound_loop(file_path, stop_event):\n",
    "    while not stop_event.is_set():\n",
    "        winsound.PlaySound(file_path, winsound.SND_FILENAME | winsound.SND_LOOP)\n",
    "        stop_event.wait(2)\n",
    "        print(\"Sound playing...\")\n",
    "    print(\"Sound stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Mediapipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.6, min_tracking_confidence=0.6)\n",
    "\n",
    "# Load Haar Cascade classifiers\n",
    "frontal_face_classifier = cv2.CascadeClassifier(\"frontal_face_detection_openCV_Github_file.xml\")\n",
    "profile_face_classifier = cv2.CascadeClassifier(\"profile_side_face_detection_openCV_Github_file.xml\")\n",
    "\n",
    "# Function to calculate depth difference\n",
    "def calculate_depth_difference(shoulder, ear):\n",
    "    return abs(ear[2] - shoulder[2])  # Depth difference (Z-coordinate)\n",
    "\n",
    "# Function to estimate ear positions from face detection\n",
    "def estimate_ear_positions(face_rect, frame_width, frame_height):\n",
    "    x, y, w, h = face_rect\n",
    "    # Estimate ear positions relative to the face rectangle\n",
    "    left_ear = (x + int(w * 0.2), y + int(h * 0.5))  # 20% from the left edge, 50% from the top\n",
    "    right_ear = (x + int(w * 0.8), y + int(h * 0.5))  # 80% from the left edge, 50% from the top\n",
    "    return left_ear, right_ear\n",
    "\n",
    "# Variables for sound control\n",
    "stop_event = threading.Event()\n",
    "sound_thread = threading.Thread(target=play_sound_loop, args=(\"beep one second.wav\", stop_event))\n",
    "sound_thread.start()\n",
    "# sound_thread = None\n",
    "\n",
    "\n",
    "# Capture webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  # Reduce frame width for faster processing\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)  # Reduce frame height for faster processing\n",
    "cap.set(cv2.CAP_PROP_FPS, 15)  # Lower frame rate to 15 FPS\n",
    "\n",
    "print(\"Press 'Q' to stop the program.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB for Mediapipe and grayscale for Haar Cascade\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces using Haar Cascade classifiers\n",
    "    frontal_faces = frontal_face_classifier.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    profile_faces = profile_face_classifier.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Process the frame with Mediapipe Pose\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Initialize ear positions\n",
    "    left_ear = None\n",
    "    right_ear = None\n",
    "\n",
    "    # Use Mediapipe ear landmarks if available\n",
    "    if results.pose_landmarks:\n",
    "        left_ear = [results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EAR].x,\n",
    "                    results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EAR].y,\n",
    "                    results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EAR].z]\n",
    "        right_ear = [results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EAR].x,\n",
    "                     results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EAR].y,\n",
    "                     results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EAR].z]\n",
    "\n",
    "    # Fallback to Haar Cascade face detection if Mediapipe fails to detect ears\n",
    "    if left_ear is None or right_ear is None:\n",
    "        if len(frontal_faces) > 0:\n",
    "            # Use the first detected frontal face\n",
    "            left_ear, right_ear = estimate_ear_positions(frontal_faces[0], frame.shape[1], frame.shape[0])\n",
    "        elif len(profile_faces) > 0:\n",
    "            # Use the first detected profile face\n",
    "            left_ear, right_ear = estimate_ear_positions(profile_faces[0], frame.shape[1], frame.shape[0])\n",
    "\n",
    "    # Analyze posture if ear positions are available\n",
    "    if left_ear is not None and right_ear is not None and results.pose_landmarks:\n",
    "        # Get keypoints for shoulders (including Z-coordinate)\n",
    "        left_shoulder = [results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].x,\n",
    "                         results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].y,\n",
    "                         results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].z]\n",
    "        right_shoulder = [results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].x,\n",
    "                          results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].y,\n",
    "                          results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].z]\n",
    "\n",
    "        # Calculate depth difference between shoulders and ears\n",
    "        left_diff = calculate_depth_difference(left_shoulder, left_ear)\n",
    "        right_diff = calculate_depth_difference(right_shoulder, right_ear)\n",
    "\n",
    "        # Classify posture\n",
    "        threshold = 0.35  # Adjust this threshold based on testing\n",
    "        if left_diff > threshold or right_diff > threshold:\n",
    "            posture = f\" bad : L {left_diff:.2f} and R {right_diff:.2f}\"\n",
    "            color = (0, 0, 255)  # Red\n",
    "            #if sound_thread is None:  # Start sound if not already playing\n",
    "            stop_event.clear()\n",
    "                # sound_thread = threading.Thread(target=play_sound_loop, args=(\"beep one second.wav\", stop_event))\n",
    "                # sound_thread.start()\n",
    "            print(\"Sound started: Bad posture detected.\")\n",
    "\n",
    "        else:\n",
    "            posture = f\"good : L {left_diff:.2f} and R {right_diff:.2f}\"\n",
    "            color = (0, 255, 0)  # Green\n",
    "            #if sound_thread is not None:  # Stop sound if playing\n",
    "            stop_event.set()\n",
    "                # sound_thread.join()\n",
    "                # sound_thread = None\n",
    "            print(\"Sound stopped: Posture corrected.\")\n",
    "\n",
    "        # Display posture feedback\n",
    "        cv2.putText(frame, posture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Draw circles on keypoints for visualization\n",
    "        cv2.circle(frame, (int(left_shoulder[0] * frame.shape[1]), int(left_shoulder[1] * frame.shape[0])), 5, (255, 0, 0), -1)\n",
    "        cv2.circle(frame, (int(right_shoulder[0] * frame.shape[1]), int(right_shoulder[1] * frame.shape[0])), 5, (255, 0, 0), -1)\n",
    "        cv2.circle(frame, (int(left_ear[0] * frame.shape[1]), int(left_ear[1] * frame.shape[0])), 5, (0, 0, 255), -1)\n",
    "        cv2.circle(frame, (int(right_ear[0] * frame.shape[1]), int(right_ear[1] * frame.shape[0])), 5, (0, 0, 255), -1)\n",
    "\n",
    "        \n",
    "    # Show the frame\n",
    "    cv2.imshow('Posture Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Check if 'Q' is pressed\n",
    "    # if keyboard.is_pressed('q'):\n",
    "    #     print(\"Program stopped by user.\")\n",
    "    #     break\n",
    "\n",
    "# Clean up\n",
    "if sound_thread is not None:\n",
    "    stop_event.set()\n",
    "    sound_thread.join(timeout=5)\n",
    "    sound_thread = None\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewEnvByMeYN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
